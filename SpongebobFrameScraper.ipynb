{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad511ef6-205b-4950-8c19-8e5646e239dc",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This is a webscraper retrieving images from https://www.everyframeinorder.com. This website has a repository of images, including every frame of Spongebob up through Season 6. At the time of writing, it contains images for 242 episodes. A Twitter account associated with this website can be found here: https://x.com/sbframesinorder?lang=en. The website is mostly accurate, containing a few points of overlapping images, invalid/empty entries, or incomplete coverage of an episode. Requests to the website are performed asynchronously with asyncio in conjunction with httpx and the html is parsed with Beautiful Soup. The initial cells sequentially retrieve the URLs of thumbnails for all images, storing them in a dataframe and saving them in a csv. The final cell creates distinct folders for each episode, whose name matches that of the episode excluding forbidden folder name characters. It retrieves every image associated to that episode, and saves them as PNGs within the created folder. One may kill the kernel and restart it without re-downloading images. The image dimensions are 64x64. In a forthcoming project, these images will be used to train a CNN for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b7795b-aa35-4417-9c9a-c91edbb22d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import httpx\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import io\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from time import perf_counter\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ChromeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f3dfaf-80fd-4ee8-9b82-7908381d4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets a list of all the episode URLS\n",
    "options = ChromeOptions() \n",
    "options.add_argument(\"--headless=new\") #opens the webpage without opening another instance of Chrome\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"https://www.everyframeinorder.com/spongebob\")\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content, \"html.parser\") \n",
    "\n",
    "episodeURLList = []\n",
    "    \n",
    "for link in soup.find_all('a')[5::2]:#Grabs all \"URL Tails\" with format like /spongebob/Sn/Em for integers n and m\n",
    "    episodeURLList.append(\"https://www.everyframeinorder.com\" + link.get('href')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03770ed8-6009-4d3e-9ae8-0bdac2cb69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieves URLS of every thumbnail and puts it in a dataframe. Column names are episode titles.\n",
    "tic = perf_counter()\n",
    "\n",
    "thumbnailURLSdict = {}\n",
    "\n",
    "async def get_episode_thumbnail_URLS(url, client):\n",
    "    page_info = await client.get(url)\n",
    "    html = page_info.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    episodeTitle = soup.find('h1').text\n",
    "    print(\"got episode \" + episodeTitle)\n",
    "    thumbnailURLList = []\n",
    "    for line in soup.findAll(attrs={\"class\": \"d-inline frame\"}): #obtains every line in HTML with class=d-inline frame\n",
    "        paragraph = line.find(\"img\") #within that line, finds the img tag\n",
    "        thumbnailURLList.append(paragraph.get(\"data-src\"))\n",
    "\n",
    "    titleAndThumbnailDict = {episodeTitle : thumbnailURLList}\n",
    "    return titleAndThumbnailDict\n",
    "\n",
    "\n",
    "def get_tasks(client):\n",
    "    tasks = []\n",
    "    for episodeURL in episodeURLList:\n",
    "        print(\"Getting task \" + episodeURL)\n",
    "        tasks.append(asyncio.create_task(get_episode_thumbnail_URLS( episodeURL, client )))\n",
    "    return tasks\n",
    "\n",
    "\n",
    "async def get_all_thumbnail_URLS():\n",
    "    async with httpx.AsyncClient(limits=httpx.Limits(max_connections=100, max_keepalive_connections=200), timeout=None) as client:\n",
    "        tasks = get_tasks(client)\n",
    "        titleAndThumbnailDictList = await asyncio.gather(*tasks)\n",
    "        for titleAndThumbnail in titleAndThumbnailDictList:\n",
    "            thumbnailURLSdict.update(titleAndThumbnail)\n",
    "\n",
    "\n",
    "\n",
    "await get_all_thumbnail_URLS()\n",
    "\n",
    "thumbnailURLSdf = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in thumbnailURLSdict.items() ]))\n",
    "thumbnailURLSdf = thumbnailURLSdf.fillna('')\n",
    "\n",
    "toc = perf_counter()\n",
    "\n",
    "print(\"That took\",toc - tic, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d624021c-d638-46d5-8f89-9b63314fcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Puts all episode URLS into a CSV\n",
    "thumbnailURLSdf.to_csv('thumbnailURLS.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9becff5-76eb-4446-9aca-7c15df05a68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading episode URLS from CSV into Dataframe. Start here after first usage.\n",
    "thumbnailURLSdf = pd.read_csv('thumbnailURLS.csv', dtype='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff055e88-cec9-4af7-b143-656206f7ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnailURLSdf.drop(thumbnailURLSdf.columns[0], axis=1, inplace=True)\n",
    "thumbnailURLSdf.fillna(value='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c75d8-ea4b-4011-b996-e2752562e99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 400)\n",
    "thumbnailURLSdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3358c-2444-4159-9946-6809a10612ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_thumbnail(url, client, directory):\n",
    "    try:\n",
    "        page_info = await client.get(url)\n",
    "        image_content = page_info.content\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        image = image.resize((64,64))\n",
    "        file_path = Path(directory, url[-15:-4].replace(\"/\", \".\") + \".png\")\n",
    "        image.save(file_path, \"PNG\", quality=90)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e, \". Episode: \" + url[-15:-4].replace(\"/\", \".\"))\n",
    "        \n",
    "\n",
    "\n",
    "def get_tasks(client, currentColumn):\n",
    "    tasks = []\n",
    "    current_directory = Path(\"C:/Users/19368/Desktop/Gary/EveryFrameOfSpongebob/\" + currentColumn.replace(\"?\", \"\"))\n",
    "    \n",
    "    if not current_directory.exists():\n",
    "        os.mkdir(current_directory)\n",
    "    \n",
    "    checkList = os.listdir(current_directory)\n",
    "    for url in thumbnailURLSdf[currentColumn]:\n",
    "        currentImageFileName = url[-15:-4].replace(\"/\", \".\") + \".png\"\n",
    "        if currentImageFileName in checkList:#if the image is already downloaded, move onto the next one\n",
    "            continue\n",
    "        if url:#makes sure that the url isn't empty\n",
    "            tasks.append(asyncio.create_task(get_thumbnail(url , client, current_directory)))\n",
    "    return tasks\n",
    "\n",
    "\n",
    "async def get_all_thumbnails(currentColumn):\n",
    "    async with httpx.AsyncClient(limits=httpx.Limits(max_connections=32, max_keepalive_connections=100), timeout=None) as client:\n",
    "        tasks = get_tasks(client, currentColumn)\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "for column in thumbnailURLSdf.columns:\n",
    "    await get_all_thumbnails(column)\n",
    "    print(column + \" completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
